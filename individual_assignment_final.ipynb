{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0b8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import sklearn.linear_model as lm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfcd9b",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0fd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"HR_data.csv\")\n",
    "data = data.drop(\"Unnamed: 0\", axis=1)\n",
    "data = data.drop(\"Individual\", axis=1)\n",
    "data = data.drop(\"Puzzler\", axis=1)\n",
    "data = data.drop(\"Round\", axis=1)\n",
    "data = data.drop(\"Phase\", axis=1)\n",
    "data = data.drop(\"Cohort\", axis=1)\n",
    "X = data.drop(\"Frustrated\", axis=1).to_numpy().astype(np.float32)\n",
    "Y = data[\"Frustrated\"].to_numpy()\n",
    "y_val = data[\"Frustrated\"].to_numpy()\n",
    "num_classes = 11\n",
    "Y = np.eye(num_classes)[Y]\n",
    "y_val_tensor = torch.tensor(y_val)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(Y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb02607",
   "metadata": {},
   "source": [
    "# **ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60456bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Epoch 500, Train Loss: 2.1752, Test Loss: 2.0975\n",
      "Fold: 1, Epoch 1000, Train Loss: 2.0555, Test Loss: 1.9530\n",
      "Fold: 1, Epoch 1500, Train Loss: 2.0144, Test Loss: 1.9088\n",
      "Fold: 2, Epoch 500, Train Loss: 2.0744, Test Loss: 1.9868\n",
      "Fold: 2, Epoch 1000, Train Loss: 1.9976, Test Loss: 1.8874\n",
      "Fold: 2, Epoch 1500, Train Loss: 2.0221, Test Loss: 1.8562\n",
      "Fold: 3, Epoch 500, Train Loss: 2.1041, Test Loss: 2.0522\n",
      "Fold: 3, Epoch 1000, Train Loss: 1.9894, Test Loss: 1.9911\n",
      "Fold: 3, Epoch 1500, Train Loss: 1.9316, Test Loss: 1.9733\n",
      "Fold: 4, Epoch 500, Train Loss: 2.0586, Test Loss: 2.0979\n",
      "Fold: 4, Epoch 1000, Train Loss: 2.0008, Test Loss: 2.1707\n",
      "Fold: 4, Epoch 1500, Train Loss: 2.0129, Test Loss: 2.1978\n",
      "Fold: 5, Epoch 500, Train Loss: 2.1942, Test Loss: 2.1407\n",
      "Fold: 5, Epoch 1000, Train Loss: 2.0490, Test Loss: 2.0089\n",
      "Fold: 5, Epoch 1500, Train Loss: 2.0209, Test Loss: 1.9584\n",
      "Fold: 6, Epoch 500, Train Loss: 2.1188, Test Loss: 2.3296\n",
      "Fold: 6, Epoch 1000, Train Loss: 1.9958, Test Loss: 2.3325\n",
      "Fold: 6, Epoch 1500, Train Loss: 1.9580, Test Loss: 2.3560\n",
      "Fold: 7, Epoch 500, Train Loss: 2.1857, Test Loss: 2.4089\n",
      "Fold: 7, Epoch 1000, Train Loss: 2.0180, Test Loss: 2.3701\n",
      "Fold: 7, Epoch 1500, Train Loss: 1.9077, Test Loss: 2.3441\n",
      "Fold: 8, Epoch 500, Train Loss: 2.0783, Test Loss: 2.0518\n",
      "Fold: 8, Epoch 1000, Train Loss: 2.0766, Test Loss: 1.8947\n",
      "Fold: 8, Epoch 1500, Train Loss: 2.0363, Test Loss: 1.8343\n",
      "Fold: 9, Epoch 500, Train Loss: 2.0869, Test Loss: 2.1848\n",
      "Fold: 9, Epoch 1000, Train Loss: 2.0556, Test Loss: 2.1642\n",
      "Fold: 9, Epoch 1500, Train Loss: 2.0022, Test Loss: 2.2163\n",
      "Fold: 10, Epoch 500, Train Loss: 2.1616, Test Loss: 2.0242\n",
      "Fold: 10, Epoch 1000, Train Loss: 2.1508, Test Loss: 1.8149\n",
      "Fold: 10, Epoch 1500, Train Loss: 2.0121, Test Loss: 1.7748\n",
      "Fold: 11, Epoch 500, Train Loss: 2.2228, Test Loss: 2.0787\n",
      "Fold: 11, Epoch 1000, Train Loss: 2.0070, Test Loss: 1.8517\n",
      "Fold: 11, Epoch 1500, Train Loss: 2.0101, Test Loss: 1.7936\n",
      "Fold: 12, Epoch 500, Train Loss: 2.1196, Test Loss: 1.9280\n",
      "Fold: 12, Epoch 1000, Train Loss: 1.9990, Test Loss: 1.8837\n",
      "Fold: 12, Epoch 1500, Train Loss: 1.9805, Test Loss: 1.9017\n",
      "Fold: 13, Epoch 500, Train Loss: 2.0873, Test Loss: 1.9184\n",
      "Fold: 13, Epoch 1000, Train Loss: 1.9731, Test Loss: 2.0193\n",
      "Fold: 13, Epoch 1500, Train Loss: 1.9575, Test Loss: 2.0886\n",
      "Fold: 14, Epoch 500, Train Loss: 2.3161, Test Loss: 2.3563\n",
      "Fold: 14, Epoch 1000, Train Loss: 2.0464, Test Loss: 2.1822\n",
      "Fold: 14, Epoch 1500, Train Loss: 2.0052, Test Loss: 2.0627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# H is hidden dimension\n",
    "H = 5\n",
    "K = 14 # folds\n",
    "\n",
    "# Device to use for computations\n",
    "device = torch.device('cpu')\n",
    "\n",
    "test_pred_ANN = []\n",
    "CV = model_selection.KFold(n_splits=K, shuffle=False)\n",
    "for train_index, test_index in CV.split(X_tensor):\n",
    "\n",
    "    X_tensor_train = X_tensor[train_index]\n",
    "    y_tensor_train = y_val_tensor[train_index]\n",
    "\n",
    "    X_tensor_test = X_tensor[test_index]\n",
    "    y_tensor_test = y_val_tensor[test_index]\n",
    "\n",
    "    # Normalize X\n",
    "    train_mean = X_tensor_train.mean(dim=0)\n",
    "    train_std = X_tensor_train.std(dim=0)\n",
    "    train_std = torch.where(train_std == 0, torch.tensor(1.0, device=device), train_std)\n",
    "    X_tensor_train = (X_tensor_train - train_mean) / train_std\n",
    "    X_tensor_test = (X_tensor_test - train_mean) / train_std\n",
    "\n",
    "    # Define model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(6, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, 11),\n",
    "        nn.Dropout(0.3)\n",
    "    )\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # DataLoader for training\n",
    "    batch_size = 32\n",
    "    dataset = TensorDataset(X_tensor_train, y_tensor_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Number of iterations and early stopping\n",
    "    T = 1500\n",
    "    Loss_train = np.zeros(T)\n",
    "    Loss_test = np.zeros(T)\n",
    "    Acc_train = []\n",
    "    Acc_test = []\n",
    "    best_test_loss = float('inf')\n",
    "    patience = 50\n",
    "    trigger_times = 0\n",
    "\n",
    "    # Training loop\n",
    "    for t in range(T):\n",
    "        epoch_loss_train = 0.0\n",
    "        model.train()\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)  # y_batch is now class indices\n",
    "            epoch_loss_train += loss.item() * X_batch.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Loss_train[t] = epoch_loss_train / len(dataset)\n",
    "        \n",
    "        # Compute test loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_tensor_test)\n",
    "            loss_test = loss_fn(y_pred_test, y_tensor_test)\n",
    "            Loss_test[t] = loss_test.item() if not torch.isnan(loss_test).any() else np.nan\n",
    "        \n",
    "        if (t + 1) % 500 == 0:\n",
    "            print(f'Fold: {test_index[0]//12 + 1}, Epoch {t+1}, Train Loss: {Loss_train[t]:.4f}, Test Loss: {Loss_test[t]:.4f}')\n",
    "\n",
    "    # Predictions:\n",
    "    for val in range(len(X_tensor_test)):\n",
    "        test_pred_ANN.append(np.argmax(model(X_tensor_test).tolist()[val]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90d8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from trained ANN (42 correct)\n",
    "test_pred_ANN = [1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9d3e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------TEST---------------\n",
      "Correct 42\n",
      "Accuracy 0.25\n"
     ]
    }
   ],
   "source": [
    "counter_ANN = 0\n",
    "for i in range(len(test_pred_ANN)):\n",
    "    counter_ANN += (y_val[i] == test_pred_ANN[i])\n",
    "print(\"---------------TEST---------------\")\n",
    "print(\"Correct\", counter_ANN)\n",
    "print(\"Accuracy\", counter_ANN/len(X_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301cae35",
   "metadata": {},
   "source": [
    "## **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6520175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "\n",
    "K = 14  # folds\n",
    "device = torch.device('cpu')\n",
    "test_pred_lr = []\n",
    "\n",
    "CV = model_selection.KFold(n_splits=K, shuffle=False)\n",
    "\n",
    "# # Verify class indices\n",
    "# assert y_val_tensor.min() >= 0 and y_val_tensor.max() <= 10, \"Invalid class indices\"\n",
    "\n",
    "for train_index, test_index in CV.split(X_tensor):\n",
    "    # Split data\n",
    "    X_tensor_train = X_tensor[train_index]\n",
    "    y_tensor_train = y_val_tensor[train_index]\n",
    "    X_tensor_test = X_tensor[test_index]\n",
    "\n",
    "    # Normalize X\n",
    "    train_mean = X_tensor_train.mean(dim=0)\n",
    "    train_std = X_tensor_train.std(dim=0)\n",
    "    train_std = torch.where(train_std == 0, torch.tensor(1.0, device=device), train_std)\n",
    "    X_tensor_train = (X_tensor_train - train_mean) / train_std\n",
    "    X_tensor_test = (X_tensor_test - train_mean) / train_std\n",
    "\n",
    "    X_tensor_train_np = X_tensor_train.cpu().numpy()\n",
    "    y_tensor_train_np = y_tensor_train.cpu().numpy()\n",
    "    X_tensor_test_np = X_tensor_test.cpu().numpy()\n",
    "\n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=1000) \n",
    "    model.fit(X_tensor_train_np, y_tensor_train_np)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = model.predict(X_tensor_test_np)\n",
    "    test_pred_lr.extend(predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec6a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------TEST---------------\n",
      "Correct 29\n",
      "Accuracy 0.17261904761904762\n"
     ]
    }
   ],
   "source": [
    "counter_lr = 0\n",
    "for i in range(len(test_pred_lr)):\n",
    "    counter_lr += (y_val[i] == test_pred_lr[i])\n",
    "print(\"---------------TEST---------------\")\n",
    "print(\"Correct\", counter_lr)\n",
    "print(\"Accuracy\", counter_lr/len(X_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7024cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = np.ones(168)\n",
    "counter_baseline = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df4af0",
   "metadata": {},
   "source": [
    "# **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c59e64",
   "metadata": {},
   "source": [
    "### **Confidence intervals (One sample)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9009079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for accuracy of ANN: [0.1906, 0.3206]\n",
      "95% CI for accuracy of LR: [0.1230, 0.2369]\n",
      "95% CI for accuracy of Baseline: [0.2066, 0.3395]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "n = len(X)\n",
    "\n",
    "# ANN\n",
    "ci_lower, ci_upper = proportion_confint(counter_ANN ,n,method=\"wilson\", alpha=alpha)\n",
    "print(f\"95% CI for accuracy of ANN: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# LR\n",
    "ci_lower, ci_upper = proportion_confint(counter_lr ,n,method=\"wilson\", alpha=alpha)\n",
    "print(f\"95% CI for accuracy of LR: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Baseline\n",
    "ci_lower, ci_upper = proportion_confint(counter_baseline ,n,method=\"wilson\", alpha=alpha)\n",
    "print(f\"95% CI for accuracy of Baseline: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cecb49",
   "metadata": {},
   "source": [
    "### **McNemar**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad239898",
   "metadata": {},
   "source": [
    "#### **ANN vs. LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be63b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "[[ 21  21]\n",
      " [  8 118]]\n",
      "McNemar p-value (ANN vs LR): 0.0241\n",
      "95% CI for difference in proportions (ANN - LR): [0.0157, 0.1391]\n"
     ]
    }
   ],
   "source": [
    "# Compute the contingency table\n",
    "table = np.array([\n",
    "    [np.sum((y_val == test_pred_ANN) & (y_val == test_pred_lr)),  # Both correct\n",
    "     np.sum((y_val == test_pred_ANN) & (y_val != test_pred_lr))],  # ANN correct, LR wrong\n",
    "    [np.sum((y_val != test_pred_ANN) & (y_val == test_pred_lr)),  # ANN wrong, LR correct\n",
    "     np.sum((y_val != test_pred_ANN) & (y_val != test_pred_lr))]   # Both wrong\n",
    "])\n",
    "\n",
    "# Print the contingency table for debugging\n",
    "print(\"Contingency Table:\")\n",
    "print(table)\n",
    "\n",
    "# Extract discordant pairs\n",
    "b = table[0, 1]  # ANN correct, LR wrong\n",
    "c = table[1, 0]  # ANN wrong, LR correct\n",
    "n = len(y_val)   # Total number of samples\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f'McNemar p-value (ANN vs LR): {result.pvalue:.4f}')\n",
    "\n",
    "# Calculate CI for the difference in proportions with continuity correction\n",
    "diff = (b - c) / n  # Difference in proportions\n",
    "z = norm.ppf(0.975)  # 97.5th percentile for 95% CI (two-tailed)\n",
    "ci_lower = diff - z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "ci_upper = diff + z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "\n",
    "print(f'95% CI for difference in proportions (ANN - LR): [{ci_lower:.4f}, {ci_upper:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da2001",
   "metadata": {},
   "source": [
    "#### **ANN vs. Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ef50ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "[[ 32  10]\n",
      " [ 13 113]]\n",
      "McNemar p-value (ANN vs baseline): 0.6776\n",
      "95% CI for difference in proportions (ANN - Baseline): [-0.0737, 0.0380]\n"
     ]
    }
   ],
   "source": [
    "# Compute the contingency table\n",
    "table = np.array([\n",
    "    [np.sum((y_val == test_pred_ANN) & (y_val == baseline)),  # Both correct\n",
    "     np.sum((y_val == test_pred_ANN) & (y_val != baseline))],  # ANN correct, LR wrong\n",
    "    [np.sum((y_val != test_pred_ANN) & (y_val == baseline)),  # ANN wrong, LR correct\n",
    "     np.sum((y_val != test_pred_ANN) & (y_val != baseline))]   # Both wrong\n",
    "])\n",
    "\n",
    "# Print the contingency table for debugging\n",
    "print(\"Contingency Table:\")\n",
    "print(table)\n",
    "\n",
    "# Extract discordant pairs\n",
    "b = table[0, 1]  # ANN correct, LR wrong\n",
    "c = table[1, 0]  # ANN wrong, LR correct\n",
    "n = len(y_val)   # Total number of samples\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f'McNemar p-value (ANN vs baseline): {result.pvalue:.4f}')\n",
    "\n",
    "# Calculate CI for the difference in proportions with continuity correction\n",
    "diff = (b - c) / n  # Difference in proportions\n",
    "z = norm.ppf(0.975)  # 97.5th percentile for 95% CI (two-tailed)\n",
    "ci_lower = diff - z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "ci_upper = diff + z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "\n",
    "print(f'95% CI for difference in proportions (ANN - Baseline): [{ci_lower:.4f}, {ci_upper:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8eea3",
   "metadata": {},
   "source": [
    "#### **LR vs. Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42938a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "[[ 17  12]\n",
      " [ 28 111]]\n",
      "McNemar p-value (LR vs baseline): 0.0166\n",
      "95% CI for difference in proportions (LR - Baseline): [-0.1676, -0.0229]\n"
     ]
    }
   ],
   "source": [
    "# Compute the contingency table\n",
    "table = np.array([\n",
    "    [np.sum((y_val == test_pred_lr) & (y_val == baseline)),  # Both correct\n",
    "     np.sum((y_val == test_pred_lr) & (y_val != baseline))],  # ANN correct, LR wrong\n",
    "    [np.sum((y_val != test_pred_lr) & (y_val == baseline)),  # ANN wrong, LR correct\n",
    "     np.sum((y_val != test_pred_lr) & (y_val != baseline))]   # Both wrong\n",
    "])\n",
    "\n",
    "# Print the contingency table for debugging\n",
    "print(\"Contingency Table:\")\n",
    "print(table)\n",
    "\n",
    "# Extract discordant pairs\n",
    "b = table[0, 1]  # ANN correct, LR wrong\n",
    "c = table[1, 0]  # ANN wrong, LR correct\n",
    "n = len(y_val)   # Total number of samples\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f'McNemar p-value (LR vs baseline): {result.pvalue:.4f}')\n",
    "\n",
    "# Calculate CI for the difference in proportions with continuity correction\n",
    "diff = (b - c) / n  # Difference in proportions\n",
    "z = norm.ppf(0.975)  # 97.5th percentile for 95% CI (two-tailed)\n",
    "ci_lower = diff - z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "ci_upper = diff + z*np.sqrt((b+c-(b-c)**2/n)/(n**2))\n",
    "\n",
    "print(f'95% CI for difference in proportions (LR - Baseline): [{ci_lower:.4f}, {ci_upper:.4f}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
